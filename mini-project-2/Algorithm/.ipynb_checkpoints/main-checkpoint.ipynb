{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932b7e50-1dfd-41cf-8262-3c6481881b07",
   "metadata": {},
   "source": [
    "## ignoring warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07d70df-6c69-4f9f-99f6-187a900aab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e1431-5625-4e33-b9cf-8fc1a6ea1f4c",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9c220d-fa56-4b60-a2bb-f3a4c3f4c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.load('../dataset/dataset/part_one_dataset/train_data/1_train_data.tar.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d063a-f6b2-49d6-a576-f0ee4e60197b",
   "metadata": {},
   "source": [
    "## Analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39512559-c29e-4b37-b5e3-ff3605f71cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'targets'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34fa842-ec9f-49b1-971d-904e79fee36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 32, 32, 3)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "print(data['data'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b97c97-c355-4b1a-97f0-c427959faa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634371-e639-4c7a-a4c4-f0761ae36de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275f45e-eec4-457c-bcea-ad7f5094b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['data'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb2040-71ab-42bd-a35e-eae62c1d0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284106d8-8076-46a9-a127-e399ca9c9e9f",
   "metadata": {},
   "source": [
    "## Converting dict to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047b0a2-4aa2-4a8e-b600-ba7e54f741f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data['targets']))\n",
    "print(type(data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163e980-9a6e-4330-9394-d48d9c572333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['targets'].shape)\n",
    "print(data['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f1c26-9d37-4bc7-9b83-7c23efea4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['targets'])\n",
    "df.columns = ['targets']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f38be8-c2da-41f9-b020-7fb812942b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f025950-82d2-4b38-97eb-978f7cbf4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='targets', data=df)\n",
    "\n",
    "# Display the plot\n",
    "plt.title('Frequency of Values in Column1')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf53e6-8b8e-4fda-be3d-3c068c59a153",
   "metadata": {},
   "source": [
    "## Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a26b4d-1b9e-4c06-bf3b-39dcde1cc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Example data (replace this with your actual data)\n",
    "X_train = data['data']  # 2500 images, 32x32x3 shape\n",
    "y_train = data['targets']   # 10 classes (0-9)\n",
    "\n",
    "# Create a CNN model for feature extraction\n",
    "input_layer = layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Flatten the output of the last convolutional layer\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Fully connected layers (optional for classification)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output_layer = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Build the model\n",
    "cnn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Perform a dummy pass to initialize the model and its weights\n",
    "cnn_model.predict(np.random.rand(1, 32, 32, 3))  # A dummy pass to initialize the model\n",
    "\n",
    "# Now, remove the classification layers and only output the features\n",
    "# Access the last convolutional layer (before flattening) for feature extraction\n",
    "feature_extractor = Model(inputs=cnn_model.input, outputs=cnn_model.layers[2].output)  # The second conv layer is at index 2\n",
    "\n",
    "# Extract features from the CNN model (this will be a 2D array of shape (2500, num_features))\n",
    "features = feature_extractor.predict(X_train)\n",
    "\n",
    "print(f\"Extracted Features Shape: {features.shape}\")  # Should print the shape like (2500, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b829c98-1985-4a2d-bc25-f47f484e91ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2e909-c43b-4b21-a1cc-1d557ea97d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac5dea-c65d-414d-b5ea-5a7745642a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52ccf9-c63c-4b5d-abfe-86dbed815d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "# Now, 'features_flat' contains the extracted features\n",
    "print(features_flat.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffaa002-f80b-4d5e-867a-e3d03ade6026",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1bbcb0-d58c-4fa1-b2ee-bd90cda9e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume features_flat is your flattened feature array from the pretrained model\n",
    "# For example, features_flat.shape = (2500, 25088) for VGG16\n",
    "\n",
    "# Initialize PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "features_reduced = pca.fit_transform(features_flat)\n",
    "\n",
    "print(features_reduced.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abe3ce-2411-4a2b-9ea4-747a3e543c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(features_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc79f07-e636-4124-b597-8196584948b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_reduced[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bef4ed-13e4-44d7-9630-2f25bed36f81",
   "metadata": {},
   "source": [
    "## Converting to Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb619c-86dd-454a-8859-d793e9a7614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the reduced feature set (features_reduced)\n",
    "features_standardized = scaler.fit_transform(features_reduced)\n",
    "\n",
    "# Now, 'features_standardized' is standardized and ready for use\n",
    "\n",
    "print(features_standardized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ba307-6cd0-4434-bf5c-f35eefaf7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(features_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d8b74-6880-495b-8664-b8bb4012b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f180cf6-310c-4f53-9396-314c9648fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_data,df],axis = 1)\n",
    "print(df_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48358dff-ae8e-4ed2-835e-403d6cbb06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((features_standardized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65920f46-9e22-438b-a202-9d59caf0885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data['targets']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f28de-edf1-477a-8545-d38b512fa665",
   "metadata": {},
   "source": [
    "## DO the lwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1c901-773c-4194-ad7a-9b556dda8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = data['targets']\n",
    "X = features_standardized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12788b7-5363-44a5-81d3-687a19ebd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example: X is your input data (already transformed, standardized, and linearized)\n",
    "# y is your labels (integers from 0 to 9 for 10 classes)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data to fit LSTM input requirements (LSTM expects 3D input)\n",
    "# Here, we reshape X_train and X_test from shape (num_samples, num_features) to (num_samples, timesteps, features)\n",
    "# We can consider each feature as a timestep, with 1 feature per timestep (timesteps=1).\n",
    "\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # (num_samples, timesteps=1, num_features)\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))      # (num_samples, timesteps=1, num_features)\n",
    "\n",
    "# Build the neural network model with an LSTM layer\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer (since X has already been transformed, we start with the feature size directly)\n",
    "model.add(LSTM(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), activation='relu', return_sequences=False))  # LSTM layer\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation='relu'))  # Second hidden layer\n",
    "model.add(Dense(64, activation='softmax'))  # Output layer with 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_prob = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Get the predicted class (the class with the highest probability)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91321ed-2dbe-4e9e-a84a-2b20396d2beb",
   "metadata": {},
   "source": [
    "### Analyzing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954061a-93b4-4607-980d-c3dc5163fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89984fa9-8278-4d05-8f79-0d7c36377bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2500):\n",
    "    img = Image.fromarray(data['data'][i])\n",
    "    target_dir = f\"./{data['targets'][i]}\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    img.save(f\"{target_dir}/img{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72dd15-ad93-4c59-ba23-553b54d6e14d",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ae71f-4108-4f59-b296-324de12f9a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │              \u001b[38;5;34m52\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │             \u001b[38;5;34m136\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,142</span> (262.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,142\u001b[0m (262.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,142</span> (262.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,142\u001b[0m (262.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.1282 - loss: 2.2977 - val_accuracy: 0.1780 - val_loss: 2.2549\n",
      "Epoch 2/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2378 - loss: 2.2140 - val_accuracy: 0.2420 - val_loss: 2.1133\n",
      "Epoch 3/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2825 - loss: 2.0462 - val_accuracy: 0.2940 - val_loss: 1.9643\n",
      "Epoch 4/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3317 - loss: 1.9036 - val_accuracy: 0.3420 - val_loss: 1.8921\n",
      "Epoch 5/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3815 - loss: 1.7995 - val_accuracy: 0.3700 - val_loss: 1.8455\n",
      "Epoch 6/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4007 - loss: 1.7468 - val_accuracy: 0.3340 - val_loss: 1.8753\n",
      "Epoch 7/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4015 - loss: 1.7323 - val_accuracy: 0.3620 - val_loss: 1.7961\n",
      "Epoch 8/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4388 - loss: 1.6375 - val_accuracy: 0.3980 - val_loss: 1.7746\n",
      "Epoch 9/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4513 - loss: 1.6241 - val_accuracy: 0.3900 - val_loss: 1.7378\n",
      "Epoch 10/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4679 - loss: 1.5506 - val_accuracy: 0.3780 - val_loss: 1.7308\n",
      "Epoch 11/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4788 - loss: 1.5084 - val_accuracy: 0.3940 - val_loss: 1.7292\n",
      "Epoch 12/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5053 - loss: 1.4692 - val_accuracy: 0.4200 - val_loss: 1.7260\n",
      "Epoch 13/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5178 - loss: 1.4214 - val_accuracy: 0.3880 - val_loss: 1.7050\n",
      "Epoch 14/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5227 - loss: 1.4153 - val_accuracy: 0.3920 - val_loss: 1.6939\n",
      "Epoch 15/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5038 - loss: 1.3801 - val_accuracy: 0.4080 - val_loss: 1.6958\n",
      "Epoch 16/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5442 - loss: 1.3003 - val_accuracy: 0.4060 - val_loss: 1.6874\n",
      "Epoch 17/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5735 - loss: 1.2785 - val_accuracy: 0.3920 - val_loss: 1.7130\n",
      "Epoch 18/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5865 - loss: 1.2137 - val_accuracy: 0.4080 - val_loss: 1.7031\n",
      "Epoch 19/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5825 - loss: 1.1901 - val_accuracy: 0.3880 - val_loss: 1.7400\n",
      "Epoch 20/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6016 - loss: 1.1726 - val_accuracy: 0.4100 - val_loss: 1.6915\n",
      "Epoch 21/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6052 - loss: 1.1608 - val_accuracy: 0.3880 - val_loss: 1.7493\n",
      "Epoch 22/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6202 - loss: 1.1184 - val_accuracy: 0.4140 - val_loss: 1.7437\n",
      "Epoch 23/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6316 - loss: 1.0851 - val_accuracy: 0.3940 - val_loss: 1.7182\n",
      "Epoch 24/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6495 - loss: 1.0249 - val_accuracy: 0.4060 - val_loss: 1.7688\n",
      "Epoch 25/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6711 - loss: 0.9859 - val_accuracy: 0.4200 - val_loss: 1.7434\n",
      "Epoch 26/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6855 - loss: 0.9673 - val_accuracy: 0.4120 - val_loss: 1.7314\n",
      "Epoch 27/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6789 - loss: 0.9443 - val_accuracy: 0.4000 - val_loss: 1.8299\n",
      "Epoch 28/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7139 - loss: 0.9052 - val_accuracy: 0.4360 - val_loss: 1.7838\n",
      "Epoch 29/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7028 - loss: 0.8942 - val_accuracy: 0.4160 - val_loss: 1.8346\n",
      "Epoch 30/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7380 - loss: 0.8483 - val_accuracy: 0.4180 - val_loss: 1.8281\n",
      "Epoch 31/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7439 - loss: 0.7967 - val_accuracy: 0.4440 - val_loss: 1.8204\n",
      "Epoch 32/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7418 - loss: 0.8047 - val_accuracy: 0.4400 - val_loss: 1.8118\n",
      "Epoch 33/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7769 - loss: 0.7260 - val_accuracy: 0.4300 - val_loss: 1.8688\n",
      "Epoch 34/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7810 - loss: 0.6993 - val_accuracy: 0.4260 - val_loss: 1.8920\n",
      "Epoch 35/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8106 - loss: 0.6716 - val_accuracy: 0.4340 - val_loss: 1.9155\n",
      "Epoch 36/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7993 - loss: 0.6399 - val_accuracy: 0.4080 - val_loss: 1.9238\n",
      "Epoch 37/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8153 - loss: 0.6232 - val_accuracy: 0.4300 - val_loss: 1.9510\n",
      "Epoch 38/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8268 - loss: 0.5676 - val_accuracy: 0.3800 - val_loss: 2.0715\n",
      "Epoch 39/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7975 - loss: 0.6119 - val_accuracy: 0.4380 - val_loss: 1.9824\n",
      "Epoch 40/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8234 - loss: 0.5823 - val_accuracy: 0.4280 - val_loss: 2.0292\n",
      "Epoch 41/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8372 - loss: 0.5455 - val_accuracy: 0.4320 - val_loss: 2.0436\n",
      "Epoch 42/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8677 - loss: 0.4925 - val_accuracy: 0.4240 - val_loss: 2.0944\n",
      "Epoch 43/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8543 - loss: 0.5040 - val_accuracy: 0.4040 - val_loss: 2.1028\n",
      "Epoch 44/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8584 - loss: 0.4750 - val_accuracy: 0.4220 - val_loss: 2.1788\n",
      "Epoch 45/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8811 - loss: 0.4521 - val_accuracy: 0.4260 - val_loss: 2.1574\n",
      "Epoch 46/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8927 - loss: 0.4043 - val_accuracy: 0.4180 - val_loss: 2.1474\n",
      "Epoch 47/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9045 - loss: 0.3814 - val_accuracy: 0.4300 - val_loss: 2.1879\n",
      "Epoch 48/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9094 - loss: 0.3747 - val_accuracy: 0.4180 - val_loss: 2.2470\n",
      "Epoch 49/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9075 - loss: 0.3517 - val_accuracy: 0.4240 - val_loss: 2.2848\n",
      "Epoch 50/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9285 - loss: 0.3129 - val_accuracy: 0.4320 - val_loss: 2.2922\n",
      "Epoch 51/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9362 - loss: 0.3021 - val_accuracy: 0.4100 - val_loss: 2.3577\n",
      "Epoch 52/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9344 - loss: 0.3015 - val_accuracy: 0.4320 - val_loss: 2.3489\n",
      "Epoch 53/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9415 - loss: 0.2616 - val_accuracy: 0.4200 - val_loss: 2.4268\n",
      "Epoch 54/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9198 - loss: 0.2942 - val_accuracy: 0.4380 - val_loss: 2.4019\n",
      "Epoch 55/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9581 - loss: 0.2390 - val_accuracy: 0.4340 - val_loss: 2.4771\n",
      "Epoch 56/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9499 - loss: 0.2326 - val_accuracy: 0.4260 - val_loss: 2.5183\n",
      "Epoch 57/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9554 - loss: 0.2157 - val_accuracy: 0.4220 - val_loss: 2.5790\n",
      "Epoch 58/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9652 - loss: 0.2006 - val_accuracy: 0.4360 - val_loss: 2.5730\n",
      "Epoch 59/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9701 - loss: 0.1898 - val_accuracy: 0.4140 - val_loss: 2.6371\n",
      "Epoch 60/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9734 - loss: 0.1736 - val_accuracy: 0.4300 - val_loss: 2.6044\n",
      "Epoch 61/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9761 - loss: 0.1665 - val_accuracy: 0.4160 - val_loss: 2.7530\n",
      "Epoch 62/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9613 - loss: 0.1811 - val_accuracy: 0.4180 - val_loss: 2.7365\n",
      "Epoch 63/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9739 - loss: 0.1621 - val_accuracy: 0.4180 - val_loss: 2.7680\n",
      "Epoch 64/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9845 - loss: 0.1324 - val_accuracy: 0.4360 - val_loss: 2.7755\n",
      "Epoch 65/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9826 - loss: 0.1214 - val_accuracy: 0.4300 - val_loss: 2.8758\n",
      "Epoch 66/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9821 - loss: 0.1222 - val_accuracy: 0.4100 - val_loss: 2.8666\n",
      "Epoch 67/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9926 - loss: 0.0966 - val_accuracy: 0.4000 - val_loss: 2.9845\n",
      "Epoch 68/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9869 - loss: 0.1031 - val_accuracy: 0.4260 - val_loss: 2.9276\n",
      "Epoch 69/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9917 - loss: 0.0995 - val_accuracy: 0.3960 - val_loss: 2.9825\n",
      "Epoch 70/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9928 - loss: 0.0899 - val_accuracy: 0.4240 - val_loss: 3.0409\n",
      "Epoch 71/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9955 - loss: 0.0751 - val_accuracy: 0.4240 - val_loss: 3.0708\n",
      "Epoch 72/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9919 - loss: 0.0761 - val_accuracy: 0.4180 - val_loss: 3.1548\n",
      "Epoch 73/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9966 - loss: 0.0613 - val_accuracy: 0.4280 - val_loss: 3.1764\n",
      "Epoch 74/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9967 - loss: 0.0777 - val_accuracy: 0.4180 - val_loss: 3.1792\n",
      "Epoch 75/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9961 - loss: 0.0608 - val_accuracy: 0.4020 - val_loss: 3.2651\n",
      "Epoch 76/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 0.0544 - val_accuracy: 0.4240 - val_loss: 3.2458\n",
      "Epoch 77/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9977 - loss: 0.0513 - val_accuracy: 0.4260 - val_loss: 3.2750\n",
      "Epoch 78/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9972 - loss: 0.0515 - val_accuracy: 0.4180 - val_loss: 3.3310\n",
      "Epoch 79/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9985 - loss: 0.0453 - val_accuracy: 0.4260 - val_loss: 3.3307\n",
      "Epoch 80/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0505 - val_accuracy: 0.4080 - val_loss: 3.3779\n",
      "Epoch 81/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0403 - val_accuracy: 0.4160 - val_loss: 3.4014\n",
      "Epoch 82/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9994 - loss: 0.0374 - val_accuracy: 0.4180 - val_loss: 3.4592\n",
      "Epoch 83/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0344 - val_accuracy: 0.4160 - val_loss: 3.5120\n",
      "Epoch 84/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0361 - val_accuracy: 0.4180 - val_loss: 3.5125\n",
      "Epoch 85/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9993 - loss: 0.0298 - val_accuracy: 0.4120 - val_loss: 3.5518\n",
      "Epoch 86/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0285 - val_accuracy: 0.4120 - val_loss: 3.5789\n",
      "Epoch 87/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0270 - val_accuracy: 0.4120 - val_loss: 3.6196\n",
      "Epoch 88/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0264 - val_accuracy: 0.4220 - val_loss: 3.6152\n",
      "Epoch 89/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0219 - val_accuracy: 0.4180 - val_loss: 3.6470\n",
      "Epoch 90/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0215 - val_accuracy: 0.4160 - val_loss: 3.7138\n",
      "Epoch 91/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0225 - val_accuracy: 0.4180 - val_loss: 3.7257\n",
      "Epoch 92/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9988 - loss: 0.0222 - val_accuracy: 0.4060 - val_loss: 3.7596\n",
      "Epoch 93/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0200 - val_accuracy: 0.4160 - val_loss: 3.7969\n",
      "Epoch 94/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0197 - val_accuracy: 0.4060 - val_loss: 3.8299\n",
      "Epoch 95/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 0.4000 - val_loss: 3.9009\n",
      "Epoch 96/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0178 - val_accuracy: 0.4140 - val_loss: 3.8844\n",
      "Epoch 97/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 0.4120 - val_loss: 3.9011\n",
      "Epoch 98/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 0.4060 - val_loss: 3.9176\n",
      "Epoch 99/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0144 - val_accuracy: 0.3980 - val_loss: 3.9618\n",
      "Epoch 100/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.4040 - val_loss: 3.9856\n",
      "Epoch 101/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0142 - val_accuracy: 0.4020 - val_loss: 3.9880\n",
      "Epoch 102/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0134 - val_accuracy: 0.4120 - val_loss: 3.9943\n",
      "Epoch 103/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 0.4080 - val_loss: 4.0425\n",
      "Epoch 104/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0112 - val_accuracy: 0.4200 - val_loss: 4.0637\n",
      "Epoch 105/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 0.4060 - val_loss: 4.1041\n",
      "Epoch 106/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.4020 - val_loss: 4.1191\n",
      "Epoch 107/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 0.4040 - val_loss: 4.1691\n",
      "Epoch 108/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0096 - val_accuracy: 0.4020 - val_loss: 4.1711\n",
      "Epoch 109/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 0.4240 - val_loss: 4.1647\n",
      "Epoch 110/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0094 - val_accuracy: 0.4040 - val_loss: 4.2358\n",
      "Epoch 111/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.4060 - val_loss: 4.2325\n",
      "Epoch 112/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.4160 - val_loss: 4.2632\n",
      "Epoch 113/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.4040 - val_loss: 4.2793\n",
      "Epoch 114/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.4060 - val_loss: 4.3154\n",
      "Epoch 115/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.4100 - val_loss: 4.2970\n",
      "Epoch 116/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.4120 - val_loss: 4.3463\n",
      "Epoch 117/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.4140 - val_loss: 4.3772\n",
      "Epoch 118/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.4100 - val_loss: 4.3780\n",
      "Epoch 119/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.4040 - val_loss: 4.4089\n",
      "Epoch 120/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.4100 - val_loss: 4.4271\n",
      "Epoch 121/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.4080 - val_loss: 4.4563\n",
      "Epoch 122/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.4000 - val_loss: 4.4717\n",
      "Epoch 123/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.4200 - val_loss: 4.4709\n",
      "Epoch 124/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.4040 - val_loss: 4.5087\n",
      "Epoch 125/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.4120 - val_loss: 4.5274\n",
      "Epoch 126/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.4040 - val_loss: 4.5543\n",
      "Epoch 127/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.4080 - val_loss: 4.5622\n",
      "Epoch 128/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.4160 - val_loss: 4.5789\n",
      "Epoch 129/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.4040 - val_loss: 4.6000\n",
      "Epoch 130/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.4120 - val_loss: 4.6212\n",
      "Epoch 131/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.4000 - val_loss: 4.6482\n",
      "Epoch 132/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.4120 - val_loss: 4.6441\n",
      "Epoch 133/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.4140 - val_loss: 4.6952\n",
      "Epoch 134/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.4140 - val_loss: 4.6895\n",
      "Epoch 135/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.4100 - val_loss: 4.6942\n",
      "Epoch 136/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.4120 - val_loss: 4.7314\n",
      "Epoch 137/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.4140 - val_loss: 4.7322\n",
      "Epoch 138/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.4060 - val_loss: 4.7623\n",
      "Epoch 139/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.4100 - val_loss: 4.7626\n",
      "Epoch 140/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.4140 - val_loss: 4.7909\n",
      "Epoch 141/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.4020 - val_loss: 4.8177\n",
      "Epoch 142/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.4120 - val_loss: 4.8411\n",
      "Epoch 143/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.4020 - val_loss: 4.8423\n",
      "Epoch 144/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.4120 - val_loss: 4.8605\n",
      "Epoch 145/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.4220 - val_loss: 4.8702\n",
      "Epoch 146/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.4040 - val_loss: 4.8999\n",
      "Epoch 147/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.4080 - val_loss: 4.9139\n",
      "Epoch 148/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.4120 - val_loss: 4.9303\n",
      "Epoch 149/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.4180 - val_loss: 4.9328\n",
      "Epoch 150/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.4060 - val_loss: 4.9499\n",
      "Epoch 151/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.4140 - val_loss: 4.9650\n",
      "Epoch 152/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.4080 - val_loss: 4.9810\n",
      "Epoch 153/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.4120 - val_loss: 4.9982\n",
      "Epoch 154/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.4100 - val_loss: 5.0092\n",
      "Epoch 155/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.4040 - val_loss: 5.0362\n",
      "Epoch 156/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.4100 - val_loss: 5.0587\n",
      "Epoch 157/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4080 - val_loss: 5.0532\n",
      "Epoch 158/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4180 - val_loss: 5.0592\n",
      "Epoch 159/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.4140 - val_loss: 5.0934\n",
      "Epoch 160/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.4120 - val_loss: 5.1005\n",
      "Epoch 161/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4120 - val_loss: 5.1268\n",
      "Epoch 162/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4100 - val_loss: 5.1277\n",
      "Epoch 163/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4120 - val_loss: 5.1529\n",
      "Epoch 164/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4100 - val_loss: 5.1652\n",
      "Epoch 165/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.4060 - val_loss: 5.1937\n",
      "Epoch 166/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.4080 - val_loss: 5.2061\n",
      "Epoch 167/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.4100 - val_loss: 5.2166\n",
      "Epoch 168/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.4120 - val_loss: 5.2426\n",
      "Epoch 169/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.4060 - val_loss: 5.2495\n",
      "Epoch 170/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.4140 - val_loss: 5.2535\n",
      "Epoch 171/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.4100 - val_loss: 5.2713\n",
      "Epoch 172/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.4100 - val_loss: 5.2739\n",
      "Epoch 173/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.4120 - val_loss: 5.3022\n",
      "Epoch 174/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.4100 - val_loss: 5.3243\n",
      "Epoch 175/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.4120 - val_loss: 5.3377\n",
      "Epoch 176/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4160 - val_loss: 5.3567\n",
      "Epoch 177/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4080 - val_loss: 5.3692\n",
      "Epoch 178/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4080 - val_loss: 5.3791\n",
      "Epoch 179/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4160 - val_loss: 5.3845\n",
      "Epoch 180/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.8364e-04 - val_accuracy: 0.4140 - val_loss: 5.4121\n",
      "Epoch 181/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.4140 - val_loss: 5.4159\n",
      "Epoch 182/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.5854e-04 - val_accuracy: 0.4140 - val_loss: 5.4270\n",
      "Epoch 183/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.7109e-04 - val_accuracy: 0.4080 - val_loss: 5.4449\n",
      "Epoch 184/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.4629e-04 - val_accuracy: 0.4080 - val_loss: 5.4546\n",
      "Epoch 185/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.9715e-04 - val_accuracy: 0.4100 - val_loss: 5.4603\n",
      "Epoch 186/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.3634e-04 - val_accuracy: 0.4100 - val_loss: 5.4707\n",
      "Epoch 187/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.3287e-04 - val_accuracy: 0.4060 - val_loss: 5.5015\n",
      "Epoch 188/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.4313e-04 - val_accuracy: 0.4180 - val_loss: 5.5067\n",
      "Epoch 189/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.6595e-04 - val_accuracy: 0.4120 - val_loss: 5.5285\n",
      "Epoch 190/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.2196e-04 - val_accuracy: 0.4120 - val_loss: 5.5423\n",
      "Epoch 191/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.7884e-04 - val_accuracy: 0.4080 - val_loss: 5.5454\n",
      "Epoch 192/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.7438e-04 - val_accuracy: 0.4100 - val_loss: 5.5587\n",
      "Epoch 193/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.0719e-04 - val_accuracy: 0.4080 - val_loss: 5.5819\n",
      "Epoch 194/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.1772e-04 - val_accuracy: 0.4080 - val_loss: 5.6141\n",
      "Epoch 195/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.2215e-04 - val_accuracy: 0.4060 - val_loss: 5.5981\n",
      "Epoch 196/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.1922e-04 - val_accuracy: 0.4120 - val_loss: 5.6222\n",
      "Epoch 197/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.8255e-04 - val_accuracy: 0.4160 - val_loss: 5.6271\n",
      "Epoch 198/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.6092e-04 - val_accuracy: 0.4140 - val_loss: 5.6310\n",
      "Epoch 199/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.3754e-04 - val_accuracy: 0.4100 - val_loss: 5.6655\n",
      "Epoch 200/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.3947e-04 - val_accuracy: 0.4040 - val_loss: 5.6790\n",
      "Epoch 201/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.4462e-04 - val_accuracy: 0.4040 - val_loss: 5.6961\n",
      "Epoch 202/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.7968e-04 - val_accuracy: 0.4100 - val_loss: 5.6973\n",
      "Epoch 203/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.9539e-04 - val_accuracy: 0.4120 - val_loss: 5.7217\n",
      "Epoch 204/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.8118e-04 - val_accuracy: 0.4120 - val_loss: 5.7200\n",
      "Epoch 205/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.1167e-04 - val_accuracy: 0.4100 - val_loss: 5.7324\n",
      "Epoch 206/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.7844e-04 - val_accuracy: 0.4120 - val_loss: 5.7443\n",
      "Epoch 207/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.6431e-04 - val_accuracy: 0.4140 - val_loss: 5.7719\n",
      "Epoch 208/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.4437e-04 - val_accuracy: 0.4040 - val_loss: 5.7683\n",
      "Epoch 209/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0976e-04 - val_accuracy: 0.4100 - val_loss: 5.7933\n",
      "Epoch 210/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0885e-04 - val_accuracy: 0.4120 - val_loss: 5.7961\n",
      "Epoch 211/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.9953e-04 - val_accuracy: 0.4080 - val_loss: 5.8225\n",
      "Epoch 212/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.9522e-04 - val_accuracy: 0.4120 - val_loss: 5.8291\n",
      "Epoch 213/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.3166e-04 - val_accuracy: 0.4060 - val_loss: 5.8332\n",
      "Epoch 214/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.3835e-04 - val_accuracy: 0.4120 - val_loss: 5.8629\n",
      "Epoch 215/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.6856e-04 - val_accuracy: 0.4020 - val_loss: 5.8694\n",
      "Epoch 216/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.4434e-04 - val_accuracy: 0.4080 - val_loss: 5.8820\n",
      "Epoch 217/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.5141e-04 - val_accuracy: 0.4100 - val_loss: 5.8859\n",
      "Epoch 218/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.2480e-04 - val_accuracy: 0.4160 - val_loss: 5.8908\n",
      "Epoch 219/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.1254e-04 - val_accuracy: 0.4160 - val_loss: 5.9161\n",
      "Epoch 220/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.1261e-04 - val_accuracy: 0.4140 - val_loss: 5.9266\n",
      "Epoch 221/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.1279e-04 - val_accuracy: 0.4100 - val_loss: 5.9449\n",
      "Epoch 222/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.0063e-04 - val_accuracy: 0.4060 - val_loss: 5.9481\n",
      "Epoch 223/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.9209e-04 - val_accuracy: 0.4100 - val_loss: 5.9626\n",
      "Epoch 224/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.6460e-04 - val_accuracy: 0.4120 - val_loss: 5.9827\n",
      "Epoch 225/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.6672e-04 - val_accuracy: 0.4140 - val_loss: 5.9895\n",
      "Epoch 226/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5048e-04 - val_accuracy: 0.4180 - val_loss: 6.0023\n",
      "Epoch 227/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.3331e-04 - val_accuracy: 0.4100 - val_loss: 6.0040\n",
      "Epoch 228/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.2131e-04 - val_accuracy: 0.4060 - val_loss: 6.0221\n",
      "Epoch 229/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.3766e-04 - val_accuracy: 0.4080 - val_loss: 6.0373\n",
      "Epoch 230/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.4737e-04 - val_accuracy: 0.4100 - val_loss: 6.0522\n",
      "Epoch 231/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.3137e-04 - val_accuracy: 0.4120 - val_loss: 6.0812\n",
      "Epoch 232/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.1418e-04 - val_accuracy: 0.4160 - val_loss: 6.0719\n",
      "Epoch 233/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.0627e-04 - val_accuracy: 0.4140 - val_loss: 6.0830\n",
      "Epoch 234/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.9007e-04 - val_accuracy: 0.4120 - val_loss: 6.1040\n",
      "Epoch 235/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.0949e-04 - val_accuracy: 0.4100 - val_loss: 6.1200\n",
      "Epoch 236/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.9553e-04 - val_accuracy: 0.4100 - val_loss: 6.1279\n",
      "Epoch 237/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8269e-04 - val_accuracy: 0.4100 - val_loss: 6.1299\n",
      "Epoch 238/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.9314e-04 - val_accuracy: 0.4120 - val_loss: 6.1524\n",
      "Epoch 239/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.7289e-04 - val_accuracy: 0.4080 - val_loss: 6.1806\n",
      "Epoch 240/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.6429e-04 - val_accuracy: 0.4100 - val_loss: 6.1712\n",
      "Epoch 241/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.5408e-04 - val_accuracy: 0.4060 - val_loss: 6.2062\n",
      "Epoch 242/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3846e-04 - val_accuracy: 0.4100 - val_loss: 6.1968\n",
      "Epoch 243/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4958e-04 - val_accuracy: 0.4120 - val_loss: 6.2178\n",
      "Epoch 244/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4403e-04 - val_accuracy: 0.4100 - val_loss: 6.2340\n",
      "Epoch 245/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.3171e-04 - val_accuracy: 0.4120 - val_loss: 6.2345\n",
      "Epoch 246/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.3111e-04 - val_accuracy: 0.4060 - val_loss: 6.2519\n",
      "Epoch 247/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4249e-04 - val_accuracy: 0.4120 - val_loss: 6.2624\n",
      "Epoch 248/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.2007e-04 - val_accuracy: 0.4140 - val_loss: 6.2683\n",
      "Epoch 249/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1535e-04 - val_accuracy: 0.4080 - val_loss: 6.2821\n",
      "Epoch 250/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.1201e-04 - val_accuracy: 0.4120 - val_loss: 6.2853\n",
      "Epoch 251/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1361e-04 - val_accuracy: 0.4100 - val_loss: 6.3230\n",
      "Epoch 252/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.0842e-04 - val_accuracy: 0.4080 - val_loss: 6.3169\n",
      "Epoch 253/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9704e-04 - val_accuracy: 0.4120 - val_loss: 6.3296\n",
      "Epoch 254/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.0057e-04 - val_accuracy: 0.4100 - val_loss: 6.3498\n",
      "Epoch 255/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8801e-04 - val_accuracy: 0.4060 - val_loss: 6.3715\n",
      "Epoch 256/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8736e-04 - val_accuracy: 0.4060 - val_loss: 6.3700\n",
      "Epoch 257/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.8236e-04 - val_accuracy: 0.4140 - val_loss: 6.3808\n",
      "Epoch 258/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.8420e-04 - val_accuracy: 0.4100 - val_loss: 6.3981\n",
      "Epoch 259/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.7668e-04 - val_accuracy: 0.4160 - val_loss: 6.3972\n",
      "Epoch 260/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.6929e-04 - val_accuracy: 0.4120 - val_loss: 6.4205\n",
      "Epoch 261/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7254e-04 - val_accuracy: 0.4120 - val_loss: 6.4269\n",
      "Epoch 262/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6586e-04 - val_accuracy: 0.4060 - val_loss: 6.4390\n",
      "Epoch 263/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.7132e-04 - val_accuracy: 0.4080 - val_loss: 6.4581\n",
      "Epoch 264/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.6190e-04 - val_accuracy: 0.4080 - val_loss: 6.4729\n",
      "Epoch 265/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6508e-04 - val_accuracy: 0.4140 - val_loss: 6.4639\n",
      "Epoch 266/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5778e-04 - val_accuracy: 0.4180 - val_loss: 6.4885\n",
      "Epoch 267/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5191e-04 - val_accuracy: 0.4160 - val_loss: 6.4870\n",
      "Epoch 268/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.5067e-04 - val_accuracy: 0.4140 - val_loss: 6.5052\n",
      "Epoch 269/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.5066e-04 - val_accuracy: 0.4100 - val_loss: 6.5191\n",
      "Epoch 270/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4774e-04 - val_accuracy: 0.4060 - val_loss: 6.5364\n",
      "Epoch 271/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3327e-04 - val_accuracy: 0.4080 - val_loss: 6.5521\n",
      "Epoch 272/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3473e-04 - val_accuracy: 0.4060 - val_loss: 6.5550\n",
      "Epoch 273/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.3545e-04 - val_accuracy: 0.4080 - val_loss: 6.5751\n",
      "Epoch 274/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.3365e-04 - val_accuracy: 0.4100 - val_loss: 6.5811\n",
      "Epoch 275/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2558e-04 - val_accuracy: 0.4140 - val_loss: 6.5897\n",
      "Epoch 276/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2469e-04 - val_accuracy: 0.4060 - val_loss: 6.6084\n",
      "Epoch 277/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2299e-04 - val_accuracy: 0.4100 - val_loss: 6.6206\n",
      "Epoch 278/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1493e-04 - val_accuracy: 0.4040 - val_loss: 6.6330\n",
      "Epoch 279/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1443e-04 - val_accuracy: 0.4060 - val_loss: 6.6515\n",
      "Epoch 280/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1212e-04 - val_accuracy: 0.4140 - val_loss: 6.6558\n",
      "Epoch 281/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.1905e-04 - val_accuracy: 0.4140 - val_loss: 6.6666\n",
      "Epoch 282/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0830e-04 - val_accuracy: 0.4040 - val_loss: 6.6776\n",
      "Epoch 283/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.9868e-05 - val_accuracy: 0.4140 - val_loss: 6.6786\n",
      "Epoch 284/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0351e-04 - val_accuracy: 0.4100 - val_loss: 6.6992\n",
      "Epoch 285/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0342e-04 - val_accuracy: 0.4100 - val_loss: 6.7186\n",
      "Epoch 286/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.6713e-05 - val_accuracy: 0.4100 - val_loss: 6.7332\n",
      "Epoch 287/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0119e-04 - val_accuracy: 0.4060 - val_loss: 6.7308\n",
      "Epoch 288/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.5684e-05 - val_accuracy: 0.4080 - val_loss: 6.7335\n",
      "Epoch 289/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.7902e-05 - val_accuracy: 0.4120 - val_loss: 6.7533\n",
      "Epoch 290/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.5276e-05 - val_accuracy: 0.4080 - val_loss: 6.7622\n",
      "Epoch 291/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 9.2347e-05 - val_accuracy: 0.4120 - val_loss: 6.7786\n",
      "Epoch 292/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.9689e-05 - val_accuracy: 0.4080 - val_loss: 6.8065\n",
      "Epoch 293/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 9.3611e-05 - val_accuracy: 0.4100 - val_loss: 6.7917\n",
      "Epoch 294/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.4798e-05 - val_accuracy: 0.4120 - val_loss: 6.8020\n",
      "Epoch 295/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.3332e-05 - val_accuracy: 0.4100 - val_loss: 6.8230\n",
      "Epoch 296/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.4465e-05 - val_accuracy: 0.4100 - val_loss: 6.8333\n",
      "Epoch 297/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.4227e-05 - val_accuracy: 0.4060 - val_loss: 6.8407\n",
      "Epoch 298/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.5003e-05 - val_accuracy: 0.4120 - val_loss: 6.8728\n",
      "Epoch 299/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.8598e-05 - val_accuracy: 0.4120 - val_loss: 6.8717\n",
      "Epoch 300/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.6257e-05 - val_accuracy: 0.4100 - val_loss: 6.8668\n",
      "Epoch 301/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.8000e-05 - val_accuracy: 0.4020 - val_loss: 6.8802\n",
      "Epoch 302/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.2028e-05 - val_accuracy: 0.4080 - val_loss: 6.9041\n",
      "Epoch 303/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.5793e-05 - val_accuracy: 0.4060 - val_loss: 6.9241\n",
      "Epoch 304/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.0849e-05 - val_accuracy: 0.4060 - val_loss: 6.9208\n",
      "Epoch 305/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.9016e-05 - val_accuracy: 0.4060 - val_loss: 6.9408\n",
      "Epoch 306/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.9477e-05 - val_accuracy: 0.4040 - val_loss: 6.9508\n",
      "Epoch 307/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.5560e-05 - val_accuracy: 0.4060 - val_loss: 6.9649\n",
      "Epoch 308/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.4139e-05 - val_accuracy: 0.4060 - val_loss: 6.9826\n",
      "Epoch 309/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.4381e-05 - val_accuracy: 0.4080 - val_loss: 6.9812\n",
      "Epoch 310/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.3853e-05 - val_accuracy: 0.4080 - val_loss: 6.9843\n",
      "Epoch 311/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.0926e-05 - val_accuracy: 0.4060 - val_loss: 7.0065\n",
      "Epoch 312/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.9331e-05 - val_accuracy: 0.4100 - val_loss: 7.0020\n",
      "Epoch 313/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.8329e-05 - val_accuracy: 0.4060 - val_loss: 7.0333\n",
      "Epoch 314/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.1168e-05 - val_accuracy: 0.4060 - val_loss: 7.0415\n",
      "Epoch 315/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.6477e-05 - val_accuracy: 0.4100 - val_loss: 7.0502\n",
      "Epoch 316/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.6568e-05 - val_accuracy: 0.4060 - val_loss: 7.0591\n",
      "Epoch 317/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.4158e-05 - val_accuracy: 0.4040 - val_loss: 7.0759\n",
      "Epoch 318/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.3898e-05 - val_accuracy: 0.4000 - val_loss: 7.0830\n",
      "Epoch 319/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.4809e-05 - val_accuracy: 0.4060 - val_loss: 7.0929\n",
      "Epoch 320/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.1260e-05 - val_accuracy: 0.4080 - val_loss: 7.1033\n",
      "Epoch 321/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0984e-05 - val_accuracy: 0.4100 - val_loss: 7.1275\n",
      "Epoch 322/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.9903e-05 - val_accuracy: 0.4020 - val_loss: 7.1238\n",
      "Epoch 323/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.1497e-05 - val_accuracy: 0.4020 - val_loss: 7.1327\n",
      "Epoch 324/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.9709e-05 - val_accuracy: 0.4040 - val_loss: 7.1471\n",
      "Epoch 325/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.8769e-05 - val_accuracy: 0.4040 - val_loss: 7.1655\n",
      "Epoch 326/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.4982e-05 - val_accuracy: 0.4040 - val_loss: 7.1748\n",
      "Epoch 327/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.3979e-05 - val_accuracy: 0.4080 - val_loss: 7.1789\n",
      "Epoch 328/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.2771e-05 - val_accuracy: 0.4100 - val_loss: 7.1972\n",
      "Epoch 329/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.3814e-05 - val_accuracy: 0.4060 - val_loss: 7.2007\n",
      "Epoch 330/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.3252e-05 - val_accuracy: 0.4040 - val_loss: 7.2226\n",
      "Epoch 331/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.0954e-05 - val_accuracy: 0.4120 - val_loss: 7.2145\n",
      "Epoch 332/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.0970e-05 - val_accuracy: 0.4060 - val_loss: 7.2356\n",
      "Epoch 333/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.1447e-05 - val_accuracy: 0.4080 - val_loss: 7.2450\n",
      "Epoch 334/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.8803e-05 - val_accuracy: 0.4020 - val_loss: 7.2624\n",
      "Epoch 335/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6057e-05 - val_accuracy: 0.4100 - val_loss: 7.2597\n",
      "Epoch 336/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.8606e-05 - val_accuracy: 0.4020 - val_loss: 7.2883\n",
      "Epoch 337/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.9424e-05 - val_accuracy: 0.4060 - val_loss: 7.2941\n",
      "Epoch 338/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.7331e-05 - val_accuracy: 0.4060 - val_loss: 7.3061\n",
      "Epoch 339/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.6224e-05 - val_accuracy: 0.4040 - val_loss: 7.3201\n",
      "Epoch 340/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.4720e-05 - val_accuracy: 0.4020 - val_loss: 7.3096\n",
      "Epoch 341/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.5536e-05 - val_accuracy: 0.4020 - val_loss: 7.3380\n",
      "Epoch 342/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.1726e-05 - val_accuracy: 0.4100 - val_loss: 7.3549\n",
      "Epoch 343/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.4048e-05 - val_accuracy: 0.4020 - val_loss: 7.3456\n",
      "Epoch 344/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2470e-05 - val_accuracy: 0.4080 - val_loss: 7.3758\n",
      "Epoch 345/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.1972e-05 - val_accuracy: 0.4040 - val_loss: 7.3593\n",
      "Epoch 346/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.1293e-05 - val_accuracy: 0.4060 - val_loss: 7.3759\n",
      "Epoch 347/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.2963e-05 - val_accuracy: 0.4060 - val_loss: 7.3843\n",
      "Epoch 348/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2097e-05 - val_accuracy: 0.4040 - val_loss: 7.4041\n",
      "Epoch 349/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.8398e-05 - val_accuracy: 0.4020 - val_loss: 7.4173\n",
      "Epoch 350/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.9413e-05 - val_accuracy: 0.4060 - val_loss: 7.4291\n",
      "Epoch 351/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.9314e-05 - val_accuracy: 0.4040 - val_loss: 7.4363\n",
      "Epoch 352/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.7310e-05 - val_accuracy: 0.4060 - val_loss: 7.4498\n",
      "Epoch 353/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8860e-05 - val_accuracy: 0.4000 - val_loss: 7.4679\n",
      "Epoch 354/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.5790e-05 - val_accuracy: 0.4040 - val_loss: 7.4697\n",
      "Epoch 355/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.5940e-05 - val_accuracy: 0.4080 - val_loss: 7.4794\n",
      "Epoch 356/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.5846e-05 - val_accuracy: 0.4060 - val_loss: 7.4923\n",
      "Epoch 357/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4325e-05 - val_accuracy: 0.4060 - val_loss: 7.5016\n",
      "Epoch 358/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.4696e-05 - val_accuracy: 0.4060 - val_loss: 7.5215\n",
      "Epoch 359/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.4751e-05 - val_accuracy: 0.4080 - val_loss: 7.5214\n",
      "Epoch 360/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.3325e-05 - val_accuracy: 0.4020 - val_loss: 7.5458\n",
      "Epoch 361/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3414e-05 - val_accuracy: 0.4020 - val_loss: 7.5500\n",
      "Epoch 362/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.3190e-05 - val_accuracy: 0.4040 - val_loss: 7.5660\n",
      "Epoch 363/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.2248e-05 - val_accuracy: 0.4020 - val_loss: 7.5632\n",
      "Epoch 364/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.2439e-05 - val_accuracy: 0.4060 - val_loss: 7.5792\n",
      "Epoch 365/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1156e-05 - val_accuracy: 0.4100 - val_loss: 7.5897\n",
      "Epoch 366/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.1013e-05 - val_accuracy: 0.4040 - val_loss: 7.5999\n",
      "Epoch 367/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.0659e-05 - val_accuracy: 0.4060 - val_loss: 7.6110\n",
      "Epoch 368/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.0834e-05 - val_accuracy: 0.4020 - val_loss: 7.6254\n",
      "Epoch 369/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.1221e-05 - val_accuracy: 0.4040 - val_loss: 7.6382\n",
      "Epoch 370/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.0719e-05 - val_accuracy: 0.4040 - val_loss: 7.6479\n",
      "Epoch 371/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.1274e-05 - val_accuracy: 0.4020 - val_loss: 7.6488\n",
      "Epoch 372/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8407e-05 - val_accuracy: 0.4080 - val_loss: 7.6557\n",
      "Epoch 373/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8492e-05 - val_accuracy: 0.4040 - val_loss: 7.6825\n",
      "Epoch 374/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.7918e-05 - val_accuracy: 0.4040 - val_loss: 7.6826\n",
      "Epoch 375/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7915e-05 - val_accuracy: 0.4040 - val_loss: 7.6984\n",
      "Epoch 376/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7056e-05 - val_accuracy: 0.4020 - val_loss: 7.7045\n",
      "Epoch 377/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6776e-05 - val_accuracy: 0.4040 - val_loss: 7.7298\n",
      "Epoch 378/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7220e-05 - val_accuracy: 0.4060 - val_loss: 7.7335\n",
      "Epoch 379/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7256e-05 - val_accuracy: 0.4040 - val_loss: 7.7410\n",
      "Epoch 380/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.7033e-05 - val_accuracy: 0.4040 - val_loss: 7.7484\n",
      "Epoch 381/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.6406e-05 - val_accuracy: 0.4060 - val_loss: 7.7590\n",
      "Epoch 382/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.5442e-05 - val_accuracy: 0.4100 - val_loss: 7.7641\n",
      "Epoch 383/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5226e-05 - val_accuracy: 0.4040 - val_loss: 7.7785\n",
      "Epoch 384/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4999e-05 - val_accuracy: 0.4080 - val_loss: 7.8133\n",
      "Epoch 385/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5458e-05 - val_accuracy: 0.4020 - val_loss: 7.8162\n",
      "Epoch 386/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.5399e-05 - val_accuracy: 0.4040 - val_loss: 7.8011\n",
      "Epoch 387/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4131e-05 - val_accuracy: 0.4020 - val_loss: 7.8310\n",
      "Epoch 388/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4248e-05 - val_accuracy: 0.4020 - val_loss: 7.8480\n",
      "Epoch 389/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.3237e-05 - val_accuracy: 0.4020 - val_loss: 7.8388\n",
      "Epoch 390/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.3068e-05 - val_accuracy: 0.4060 - val_loss: 7.8584\n",
      "Epoch 391/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2914e-05 - val_accuracy: 0.4080 - val_loss: 7.8669\n",
      "Epoch 392/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2953e-05 - val_accuracy: 0.4020 - val_loss: 7.8809\n",
      "Epoch 393/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2151e-05 - val_accuracy: 0.4000 - val_loss: 7.9036\n",
      "Epoch 394/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3387e-05 - val_accuracy: 0.4020 - val_loss: 7.8913\n",
      "Epoch 395/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.1852e-05 - val_accuracy: 0.4040 - val_loss: 7.9112\n",
      "Epoch 396/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.1978e-05 - val_accuracy: 0.4040 - val_loss: 7.9196\n",
      "Epoch 397/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2147e-05 - val_accuracy: 0.4040 - val_loss: 7.9380\n",
      "Epoch 398/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2005e-05 - val_accuracy: 0.4020 - val_loss: 7.9531\n",
      "Epoch 399/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1637e-05 - val_accuracy: 0.4020 - val_loss: 7.9604\n",
      "Epoch 400/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1016e-05 - val_accuracy: 0.4020 - val_loss: 7.9651\n",
      "Epoch 401/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0790e-05 - val_accuracy: 0.4040 - val_loss: 7.9855\n",
      "Epoch 402/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0581e-05 - val_accuracy: 0.4040 - val_loss: 7.9777\n",
      "Epoch 403/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 9.9352e-06 - val_accuracy: 0.4060 - val_loss: 7.9971\n",
      "Epoch 404/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.9963e-06 - val_accuracy: 0.4060 - val_loss: 8.0122\n",
      "Epoch 405/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0760e-05 - val_accuracy: 0.4060 - val_loss: 8.0099\n",
      "Epoch 406/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0599e-05 - val_accuracy: 0.4040 - val_loss: 8.0196\n",
      "Epoch 407/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.9254e-06 - val_accuracy: 0.4020 - val_loss: 8.0428\n",
      "Epoch 408/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.6697e-06 - val_accuracy: 0.4040 - val_loss: 8.0659\n",
      "Epoch 409/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.2706e-06 - val_accuracy: 0.4040 - val_loss: 8.0670\n",
      "Epoch 410/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.2573e-06 - val_accuracy: 0.4040 - val_loss: 8.0647\n",
      "Epoch 411/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 9.4252e-06 - val_accuracy: 0.4080 - val_loss: 8.0816\n",
      "Epoch 412/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.8110e-06 - val_accuracy: 0.4040 - val_loss: 8.0758\n",
      "Epoch 413/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.4788e-06 - val_accuracy: 0.4040 - val_loss: 8.1080\n",
      "Epoch 414/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.6899e-06 - val_accuracy: 0.4080 - val_loss: 8.1160\n",
      "Epoch 415/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.7843e-06 - val_accuracy: 0.4040 - val_loss: 8.1274\n",
      "Epoch 416/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.4021e-06 - val_accuracy: 0.4020 - val_loss: 8.1385\n",
      "Epoch 417/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.6965e-06 - val_accuracy: 0.4020 - val_loss: 8.1371\n",
      "Epoch 418/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.9717e-06 - val_accuracy: 0.4020 - val_loss: 8.1448\n",
      "Epoch 419/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.9965e-06 - val_accuracy: 0.4040 - val_loss: 8.1598\n",
      "Epoch 420/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.7111e-06 - val_accuracy: 0.4020 - val_loss: 8.1774\n",
      "Epoch 421/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.5578e-06 - val_accuracy: 0.4020 - val_loss: 8.1867\n",
      "Epoch 422/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.1721e-06 - val_accuracy: 0.4040 - val_loss: 8.1854\n",
      "Epoch 423/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.1943e-06 - val_accuracy: 0.4000 - val_loss: 8.1988\n",
      "Epoch 424/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 7.2367e-06 - val_accuracy: 0.4040 - val_loss: 8.2162\n",
      "Epoch 425/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.0373e-06 - val_accuracy: 0.4040 - val_loss: 8.2367\n",
      "Epoch 426/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.7669e-06 - val_accuracy: 0.4060 - val_loss: 8.2435\n",
      "Epoch 427/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.7502e-06 - val_accuracy: 0.4080 - val_loss: 8.2606\n",
      "Epoch 428/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.6998e-06 - val_accuracy: 0.4040 - val_loss: 8.2693\n",
      "Epoch 429/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.4437e-06 - val_accuracy: 0.4060 - val_loss: 8.2687\n",
      "Epoch 430/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.7637e-06 - val_accuracy: 0.4000 - val_loss: 8.2754\n",
      "Epoch 431/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.3437e-06 - val_accuracy: 0.4040 - val_loss: 8.2958\n",
      "Epoch 432/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.9474e-06 - val_accuracy: 0.4040 - val_loss: 8.3096\n",
      "Epoch 433/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.2318e-06 - val_accuracy: 0.4040 - val_loss: 8.3089\n",
      "Epoch 434/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.7469e-06 - val_accuracy: 0.4080 - val_loss: 8.3138\n",
      "Epoch 435/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.4801e-06 - val_accuracy: 0.4100 - val_loss: 8.3345\n",
      "Epoch 436/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.6888e-06 - val_accuracy: 0.4020 - val_loss: 8.3360\n",
      "Epoch 437/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.6786e-06 - val_accuracy: 0.4080 - val_loss: 8.3500\n",
      "Epoch 438/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.4732e-06 - val_accuracy: 0.4040 - val_loss: 8.3655\n",
      "Epoch 439/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.5529e-06 - val_accuracy: 0.4040 - val_loss: 8.3826\n",
      "Epoch 440/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.0748e-06 - val_accuracy: 0.4060 - val_loss: 8.3720\n",
      "Epoch 441/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.1168e-06 - val_accuracy: 0.4020 - val_loss: 8.4055\n",
      "Epoch 442/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.1426e-06 - val_accuracy: 0.4040 - val_loss: 8.4088\n",
      "Epoch 443/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.9013e-06 - val_accuracy: 0.4060 - val_loss: 8.4273\n",
      "Epoch 444/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.8298e-06 - val_accuracy: 0.4040 - val_loss: 8.4286\n",
      "Epoch 445/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.6840e-06 - val_accuracy: 0.4040 - val_loss: 8.4513\n",
      "Epoch 446/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.4204e-06 - val_accuracy: 0.4020 - val_loss: 8.4580\n",
      "Epoch 447/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.6921e-06 - val_accuracy: 0.4040 - val_loss: 8.4686\n",
      "Epoch 448/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.7890e-06 - val_accuracy: 0.4080 - val_loss: 8.4564\n",
      "Epoch 449/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.5299e-06 - val_accuracy: 0.4060 - val_loss: 8.4937\n",
      "Epoch 450/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.4619e-06 - val_accuracy: 0.4060 - val_loss: 8.4859\n",
      "Epoch 451/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.4246e-06 - val_accuracy: 0.4040 - val_loss: 8.5076\n",
      "Epoch 452/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.2111e-06 - val_accuracy: 0.4060 - val_loss: 8.5045\n",
      "Epoch 453/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.3328e-06 - val_accuracy: 0.4060 - val_loss: 8.5137\n",
      "Epoch 454/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9849e-06 - val_accuracy: 0.4040 - val_loss: 8.5392\n",
      "Epoch 455/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.0018e-06 - val_accuracy: 0.4000 - val_loss: 8.5478\n",
      "Epoch 456/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.8544e-06 - val_accuracy: 0.4040 - val_loss: 8.5570\n",
      "Epoch 457/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9271e-06 - val_accuracy: 0.4060 - val_loss: 8.5627\n",
      "Epoch 458/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.8237e-06 - val_accuracy: 0.4060 - val_loss: 8.5615\n",
      "Epoch 459/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.8287e-06 - val_accuracy: 0.4020 - val_loss: 8.5898\n",
      "Epoch 460/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 6.0964e-08 - val_accuracy: 0.4020 - val_loss: 10.8256\n",
      "Epoch 690/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.0417e-08 - val_accuracy: 0.4020 - val_loss: 10.8231\n",
      "Epoch 691/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.1376e-08 - val_accuracy: 0.4020 - val_loss: 10.8365\n",
      "Epoch 692/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.0979e-08 - val_accuracy: 0.4020 - val_loss: 10.8430\n",
      "Epoch 693/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.5250e-08 - val_accuracy: 0.4020 - val_loss: 10.8511\n",
      "Epoch 694/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.1919e-08 - val_accuracy: 0.4000 - val_loss: 10.8620\n",
      "Epoch 695/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.9984e-08 - val_accuracy: 0.4020 - val_loss: 10.8546\n",
      "Epoch 696/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.5655e-08 - val_accuracy: 0.4020 - val_loss: 10.8717\n",
      "Epoch 697/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.0713e-08 - val_accuracy: 0.4020 - val_loss: 10.8890\n",
      "Epoch 698/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 5.0614e-08 - val_accuracy: 0.4000 - val_loss: 10.9046\n",
      "Epoch 699/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.9820e-08 - val_accuracy: 0.4020 - val_loss: 10.9066\n",
      "Epoch 700/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.4464e-08 - val_accuracy: 0.4040 - val_loss: 10.9332\n",
      "Epoch 701/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.2171e-08 - val_accuracy: 0.4000 - val_loss: 10.9422\n",
      "Epoch 702/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.0792e-08 - val_accuracy: 0.4000 - val_loss: 10.9511\n",
      "Epoch 703/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.8174e-08 - val_accuracy: 0.4020 - val_loss: 10.9607\n",
      "Epoch 704/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.6922e-08 - val_accuracy: 0.4020 - val_loss: 10.9801\n",
      "Epoch 705/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.5535e-08 - val_accuracy: 0.4000 - val_loss: 10.9909\n",
      "Epoch 706/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.9045e-08 - val_accuracy: 0.4060 - val_loss: 10.9881\n",
      "Epoch 707/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.8188e-08 - val_accuracy: 0.3960 - val_loss: 11.0073\n",
      "Epoch 708/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.5623e-08 - val_accuracy: 0.4020 - val_loss: 11.0169\n",
      "Epoch 709/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.6768e-08 - val_accuracy: 0.4020 - val_loss: 11.0186\n",
      "Epoch 710/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.5409e-08 - val_accuracy: 0.4000 - val_loss: 11.0521\n",
      "Epoch 711/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.6696e-08 - val_accuracy: 0.4000 - val_loss: 11.0546\n",
      "Epoch 712/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.3013e-08 - val_accuracy: 0.3940 - val_loss: 11.0672\n",
      "Epoch 713/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.1973e-08 - val_accuracy: 0.4000 - val_loss: 11.0980\n",
      "Epoch 714/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.2364e-08 - val_accuracy: 0.4020 - val_loss: 11.0883\n",
      "Epoch 715/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.0974e-08 - val_accuracy: 0.3980 - val_loss: 11.1038\n",
      "Epoch 716/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.0320e-08 - val_accuracy: 0.3960 - val_loss: 11.1282\n",
      "Epoch 717/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.4804e-08 - val_accuracy: 0.4040 - val_loss: 11.1467\n",
      "Epoch 718/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.0979e-08 - val_accuracy: 0.3960 - val_loss: 11.1459\n",
      "Epoch 719/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.8578e-08 - val_accuracy: 0.4060 - val_loss: 11.1620\n",
      "Epoch 720/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.2188e-08 - val_accuracy: 0.3940 - val_loss: 11.2121\n",
      "Epoch 721/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.7128e-08 - val_accuracy: 0.3980 - val_loss: 11.1905\n",
      "Epoch 722/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.7718e-08 - val_accuracy: 0.4020 - val_loss: 11.1993\n",
      "Epoch 723/10000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 4.0176e-08 - val_accuracy: 0.4000 - val_loss: 11.2252\n",
      "Epoch 724/10000\n",
      "\u001b[1m 8/32\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.3567e-08 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `images` is a NumPy array or TensorFlow tensor with shape (num_samples, 32, 32, 3)\n",
    "images = data['data']  # Update with your actual dataset\n",
    "labels = data['targets']  # Ensure you have corresponding labels for supervised learning\n",
    "\n",
    "# Normalize the image data\n",
    "images = images / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Convert labels to one-hot encoding if not already\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=10)  # Adjust `num_classes` as per your dataset\n",
    "\n",
    "# Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Enhanced CNN Model for feature extraction\n",
    "model = tf.keras.Sequential([\n",
    "    # First Convolutional Block\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=4,  # Number of filters for learning low-level features\n",
    "        kernel_size=(2, 2),  # Filter size\n",
    "        strides=(1, 1),  # No down-sampling yet\n",
    "        padding='same',  # Keeps output size same\n",
    "        activation='relu',  # ReLU activation\n",
    "        input_shape=(32, 32, 3)  # Input shape for RGB images\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2),  # Reduce height and width by 2\n",
    "        strides=(2, 2),\n",
    "        padding='valid'  # Reduce size\n",
    "    ),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=8,  # Double the filters for learning mid-level features\n",
    "        kernel_size=(2, 2),\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        activation='relu'\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        strides=(2, 2),\n",
    "        padding='valid'\n",
    "    ),\n",
    "\n",
    "    # Flatten the features to use later or pass through Dense layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Add Dense layers for classification\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Adjust `10` to your number of classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10000,  # Train for 10000 epochs\n",
    "    batch_size=64,  # Adjust batch size as needed\n",
    "    verbose=1  # Use 1 for progress bar, 2 for one line per epoch\n",
    ")\n",
    "\n",
    "# Extract features using the model\n",
    "# Use intermediate layers or predictions for features\n",
    "features = model.predict(images)\n",
    "\n",
    "# Print feature shape\n",
    "print(f\"Extracted features shape: {features.shape}\")  # Example: (num_samples, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58611761-22a5-435e-b627-5c7f35e81c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 224)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume features_flat is your flattened feature array from the pretrained model\n",
    "# For example, features_flat.shape = (2500, 25088) for VGG16\n",
    "\n",
    "# Initialize PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "features_reduced = pca.fit_transform(features)\n",
    "\n",
    "print(features_reduced.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781af6c8-e20d-484c-9c94-a6e8772f8c32",
   "metadata": {},
   "source": [
    "### Tsne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39a66d60-327d-42aa-ace8-fb4aaac4d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5387ef27-c22c-468f-882e-faccaaf2bb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'n_components' should be inferior to 4 for the barnes_hut algorithm as it relies on quad-tree or oct-tree.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m features_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a scatter plot where each class will have a different color\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=y, cmap='jet', s=50, alpha=0.7)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# # Show the plot\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1136\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;124;03m    Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m-> 1136\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:900\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSNE with method=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not accept sparse \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed distance matrix. Use method=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes_hut\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    896\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor provide the dense distance matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    897\u001b[0m         )\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes_hut\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 900\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_components\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be inferior to 4 for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes_hut algorithm as it relies on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquad-tree or oct-tree.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m     )\n\u001b[0;32m    905\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    907\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: 'n_components' should be inferior to 4 for the barnes_hut algorithm as it relies on quad-tree or oct-tree."
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=4, random_state=42)\n",
    "features_2d = tsne.fit_transform(features)\n",
    "\n",
    "# Create a scatter plot where each class will have a different color\n",
    "# scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=y, cmap='jet', s=50, alpha=0.7)\n",
    "\n",
    "# # Add a color bar\n",
    "# plt.colorbar(scatter, label='Class')\n",
    "\n",
    "# # Set plot labels and title\n",
    "# plt.xlabel('t-SNE Component 1')\n",
    "# plt.ylabel('t-SNE Component 2')\n",
    "# plt.title('t-SNE visualization of high-dimensional data')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86214c0b-009e-4032-85e3-a7e10403ee4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m umap_model \u001b[38;5;241m=\u001b[39m \u001b[43mumap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUMAP\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m features_2d \u001b[38;5;241m=\u001b[39m umap_model\u001b[38;5;241m.\u001b[39mfit_transform(features)  \u001b[38;5;66;03m# Apply UMAP\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "umap_model = umap.UMAP(n_components=20, random_state=42)\n",
    "features_2d = umap_model.fit_transform(features)  # Apply UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83704096-cb84-4b9e-84c4-c994ce8ccd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c1d6507-8a70-4c93-8932-b1282ef341a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_2d\n",
    "y = data['targets']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c10365d-06e5-4504-975b-21e0d92e179c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6d060-e847-4edd-8e99-1cab1327ebf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af311477-f53e-48db-b1f6-868ddb30adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 512)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize the input data (optional but recommended)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53fa8232-e296-4c96-858f-5299086d7640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 69.677765     0.           0.         165.73352      0.\n",
      " 176.1767      31.34619    127.75385     83.99743      0.\n",
      "   0.         229.00185      0.         169.65599     14.078581\n",
      " 155.83363     88.53352      0.           0.         244.74388\n",
      "   0.         202.96416     40.917416   146.73787     69.33796\n",
      "   0.           0.         224.71765      0.         201.28569\n",
      "  67.99785    163.4591      62.88956      0.           0.\n",
      " 229.5444       0.         183.10992     27.285147   161.21759\n",
      "  44.857487     2.6172452    0.         120.701004     0.\n",
      "  79.66397      3.984033    93.256294    27.527452     0.\n",
      "   0.          81.49945      0.          72.94427     17.175558\n",
      "  57.538002    14.803525     0.           0.          84.30511\n",
      "   0.          54.622143     7.3381886   53.97275     93.98909\n",
      "   0.           0.         193.22508      0.         126.08862\n",
      "  41.226246    97.7383      38.62818      0.           0.\n",
      " 201.33678      0.          96.200615     0.81942445 103.929756\n",
      "  78.187996     0.           0.         158.92886      0.\n",
      "  75.38312      0.          78.04417     93.01987     16.88218\n",
      "   0.         187.74965      0.         177.41753     78.87956\n",
      " 135.73222     45.639744     1.8851217    0.         219.65024\n",
      "   0.         174.57637     31.986315   158.40813     40.542923\n",
      "   0.           0.         163.01465      0.          99.903114\n",
      "   0.         127.931404    24.70409      3.1620286    0.\n",
      "  85.551476     0.          74.60562     21.182499    61.64578\n",
      "  27.218506     0.           0.          78.79623      0.\n",
      "  59.812084    14.18143     55.095863    74.70435      0.\n",
      "   0.         149.25264      0.         112.51681     17.26467\n",
      "  89.907364    42.608044     0.           0.         181.18626\n",
      "   0.         101.66399      0.         113.7718      49.017788\n",
      "   0.           0.         167.09772      0.          90.28553\n",
      "   0.          92.71146     71.104126    16.467056     0.\n",
      " 133.74168      0.         146.7094      78.288246    76.31368\n",
      "  48.947407     8.085408     0.         203.80637      0.\n",
      " 172.47214     37.04448    157.13048     34.908928    11.255907\n",
      "   0.         151.17088      0.         156.52107     67.12974\n",
      " 129.09009     40.262207     2.4757562    0.         134.03368\n",
      "   0.         113.54788      8.113485   114.67915     44.99818\n",
      "   0.           0.         125.6714       0.          70.498215\n",
      "   0.5705138   86.23934     74.14821      0.           0.\n",
      " 190.21892      0.         197.25783     41.107147   148.55692\n",
      "  50.81351      0.           0.         211.90395      0.\n",
      " 189.98853     19.690096   169.8346      68.010284     0.\n",
      "   0.         227.21227      0.         207.56702     38.07716\n",
      " 178.46606     96.42088      3.5251865    0.         226.2769\n",
      "   0.         168.94287     38.25625    170.69818     57.113354\n",
      "   3.2942762    0.         203.79236      0.         171.64914\n",
      "  26.484459   151.88712     27.849138    11.615117     0.\n",
      " 136.23131      0.         150.22531     55.562614   119.65503\n",
      "  27.044641     8.060335     0.         149.29568      0.\n",
      " 155.10054     51.65952    122.0739      20.795996     0.\n",
      "   0.         144.39575      0.         122.83898     27.08456\n",
      " 112.36031     70.95404      1.2083054    0.         276.95477\n",
      "   0.         231.31224     52.074368   193.56476     63.551098\n",
      "   0.           0.         240.3996       0.         224.35452\n",
      "  57.643753   183.46913     55.92729      0.           0.\n",
      " 242.21048      0.         198.69936     54.254242   171.63457\n",
      "  67.53949      0.           0.         251.21738      0.\n",
      " 202.41219     27.274094   174.28886     58.523228     8.379363\n",
      "   0.         241.54883      0.         182.65302     18.749247\n",
      " 165.86023     29.997952    10.1481495    0.         139.1195\n",
      "   0.         170.64444     62.199207   136.94106     34.347034\n",
      "   7.6709204    0.         142.44067      0.         173.79877\n",
      "  51.497154   140.297       25.944763     0.           0.\n",
      " 156.56923      0.         154.77911     38.41999    124.63267\n",
      "  37.402874     0.           0.         188.6212       0.\n",
      " 107.30332      0.         123.9625      65.441124     0.\n",
      "   0.         160.18144      0.         129.2075      29.35276\n",
      " 112.71752    101.10488      5.180106     0.         227.7425\n",
      "   0.         152.05142     55.172306   133.00142     75.44771\n",
      "   0.           0.         223.63263      0.         169.23834\n",
      "  57.949318   150.64896     55.49817      7.447495     0.\n",
      " 198.25732      0.         147.20241     45.450897   135.40536\n",
      "  74.835236     8.0870905    0.         164.62573      0.\n",
      " 139.48636     76.15349    104.41308     45.403038    10.573694\n",
      "   0.         156.29759      0.         181.11308     61.367558\n",
      " 147.68506     31.38565      0.           0.         182.03673\n",
      "   0.         161.257       67.064865   140.14502     47.48215\n",
      "   0.           0.         175.2867       0.         143.81015\n",
      "  25.13522    122.91853     70.97637      2.513088     0.\n",
      " 189.01056      0.         141.11496     24.526058   133.38448\n",
      "  49.173042     0.           0.         185.39047      0.\n",
      " 146.36525     26.429672   128.5957      24.338806     0.\n",
      "   0.         116.14052      0.          76.434586    10.200859\n",
      "  89.55728     83.486565     6.804088     0.         115.704956\n",
      "   0.          71.13222     18.600422    77.808556    29.375217\n",
      "   0.           0.         110.446724     0.          79.453255\n",
      "  15.470309    68.75761     53.252502     0.           0.\n",
      "  89.653786     0.          83.428894    25.988392    72.413216\n",
      "  79.807304     4.5315213    0.         117.90997      0.\n",
      "  84.92314     43.4207      76.811295    61.29922      4.6982355\n",
      "  21.32035    197.92206      0.         156.57031     72.777725\n",
      " 122.17003     26.229969     6.465784    34.143566   166.53204\n",
      "   0.         139.13603     74.53576    111.41573     32.51282\n",
      "   7.2815657   30.187695   162.08823      0.         151.58992\n",
      "  70.550476   120.261406    46.832184     0.           5.7048893\n",
      " 111.56677      0.          90.506676    22.758028    65.587616\n",
      "  28.243694     0.           3.8035717  102.49881      0.\n",
      "  53.714455    14.314245    67.22421     39.80743      5.0199466\n",
      "  32.265434    88.69997      0.          98.7866      62.56327\n",
      "  66.6753      31.927042     1.5976235   31.531015   143.39638\n",
      "   0.         125.58204     65.42092    115.73221     45.09752\n",
      "   4.1644025   23.707191   153.48578      0.         118.89492\n",
      "  72.61737    100.13825   ]\n"
     ]
    }
   ],
   "source": [
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "004b513b-dee6-477b-b7a5-e463ca964b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13.862544    14.950963     0.          67.28831      0.\n",
      "  89.52783     58.406456    61.56639     17.7618      10.0758\n",
      "   0.         107.9237       0.         101.77234     52.808384\n",
      "  80.089195    11.62138     10.526963     0.         103.072014\n",
      "   0.         100.64986     55.43944     79.60229      6.350231\n",
      "   6.716766     0.         104.08884      0.          94.10237\n",
      "  46.45564     87.55301      5.4538713    7.3767576    0.\n",
      "  97.75872      0.          99.92352     47.637486    78.77121\n",
      "  15.598895    11.029854     0.         108.19276      0.\n",
      " 108.25096     62.37186     87.24896     10.566191     7.516699\n",
      "   0.         115.85992      0.         108.86381     57.064243\n",
      "  90.218575     7.802604     0.           0.         125.28652\n",
      "   0.          98.84879     42.627308    90.30014     15.9485445\n",
      "  12.499206     0.          84.75455      0.          98.74079\n",
      "  51.255264    82.23799      4.657741     7.2165494    0.\n",
      "  74.47747      0.          92.197495    46.12319     78.0116\n",
      "  15.3274555    5.1996555    0.          85.04816      0.\n",
      "  85.41855     36.316265    79.53734      5.305396    25.370901\n",
      "   0.          84.52188      0.         137.61465     98.204216\n",
      "  84.31423     26.839998    11.60513      0.         114.5433\n",
      "   0.          91.567215    62.86661    123.284676     1.911138\n",
      "  10.842883     0.          91.26538      0.          97.96524\n",
      "  58.684315    88.58516      4.5381937    6.2719536    0.\n",
      "  86.2871       0.          82.66399     35.320004    83.62373\n",
      "   0.           0.92698133   0.          71.545616     0.\n",
      "  76.86781     30.1651      66.61387     10.900633     7.143798\n",
      "   0.         101.646355     0.         109.72552     42.693737\n",
      "  93.40261     20.818415     8.265984     0.         101.75774\n",
      "   0.         102.24385     42.940174    82.9783      23.441736\n",
      "  10.602463     0.         101.714294     0.         137.33466\n",
      "  56.943665   101.175446    37.453136    23.019892     0.\n",
      "  86.59577      0.         137.30745     89.72644    123.64078\n",
      "  33.082645    11.780367     0.         116.197        0.\n",
      " 143.54161     65.709755   107.002655    31.596298    12.016845\n",
      "   0.         122.52869      0.         102.81354     46.136105\n",
      " 114.298744     4.4008904    7.274477     0.          80.68787\n",
      "   0.          91.82619     43.07924     80.45599      3.8777797\n",
      "   2.1156585    0.          82.56448      0.          89.09605\n",
      "  38.095318    79.11286     12.11962      7.250669     0.\n",
      "  99.12556      0.         101.78906     46.097767    89.691055\n",
      "  33.958504    16.138737     0.          82.30505      0.\n",
      " 154.9591      91.38416     80.255585    55.22688     19.705643\n",
      "   0.         159.97365      0.         168.99005     91.16364\n",
      " 136.15059     41.587105     5.3667617    0.         198.439\n",
      "   0.         209.35533     84.92464    165.21135     54.835693\n",
      "   6.8229537    0.         202.64587      0.         194.98047\n",
      "  44.46679    169.02484     52.113575     4.83338      0.\n",
      " 184.661        0.         183.4525      82.11549    149.80112\n",
      "  32.220036     7.29227      0.         125.05891      0.\n",
      " 111.008125    42.230392   114.64454     11.249432     1.2949921\n",
      "   0.          88.26728      0.          90.86297     38.80392\n",
      "  80.1041       7.0077205   12.166674     0.          87.75113\n",
      "   0.         119.43905     66.09805     84.41515     74.255264\n",
      "  20.656612     0.         219.77592      0.         213.00502\n",
      " 122.3935     167.1488      39.9457      11.343844     0.\n",
      " 196.43236      0.         191.7291      96.53554    161.96086\n",
      "  55.078857     7.4257293    0.         209.42844      0.\n",
      " 195.76183     65.428856   150.85124     21.735708    16.744207\n",
      "   0.         181.36092      0.         165.54562    108.65032\n",
      " 142.19379     53.031963    12.349368     0.         195.07576\n",
      "   0.         201.98698     78.04628    166.0489      15.207001\n",
      "   5.8028855    0.         142.38866      0.          99.5856\n",
      "  47.018143   119.1965       5.482156     0.           0.\n",
      "  95.73874      0.          94.681595    37.02913     78.885376\n",
      "  41.691227    19.255629     0.          99.78905      0.\n",
      " 127.52025     84.20914     90.04217     35.880444    17.598732\n",
      "   0.         167.1872       0.         158.23279    104.80734\n",
      " 126.79651     29.46479     10.986451     0.         136.09471\n",
      "   0.         171.2832      75.31815    142.3081      29.766068\n",
      "  10.109039     0.         133.36249      0.         132.01442\n",
      "  56.477245   144.11264     44.622906    18.802807     0.\n",
      " 106.03982      0.         134.49673     88.06092     90.912994\n",
      "  10.712119     9.121004     0.         151.81812      0.\n",
      " 135.98697     71.99774    111.19852     22.832111    11.693255\n",
      "   0.          93.04147      0.          97.77779     39.96609\n",
      "  95.272934    44.411324     0.           0.         123.59934\n",
      "   0.         125.42217     48.35754     96.67879     21.119698\n",
      "   8.234136     0.          98.92644      0.         103.899254\n",
      "  52.989597   107.047844    14.733735    11.337706     0.\n",
      " 107.75139      0.         117.75344     64.63766     91.12726\n",
      "  30.47971     14.609983     0.         150.39503      0.\n",
      " 133.82788     83.50723    101.01413      4.6490965    9.987411\n",
      "   0.         156.33212      0.         106.128044    54.69084\n",
      " 113.572914    15.504841    10.328533     0.          92.74911\n",
      "   0.         106.42745     54.485638    92.67715     10.219367\n",
      "   8.518259     0.          91.66496      0.         101.560394\n",
      "  48.05693     88.70034     33.09023     17.009272     0.\n",
      " 117.37199      0.         144.3266      76.607956   102.68295\n",
      "  23.498009     0.           0.         172.28987      0.\n",
      " 127.108406    27.132479   121.73421     24.86655     11.360768\n",
      "  42.969364   156.38103      0.         173.36008     93.83316\n",
      " 147.6106      50.23019     14.897058    32.136173   167.08128\n",
      "   0.         186.87077     83.457664   145.48116     32.023754\n",
      "  11.04838     27.602875   135.902        0.         158.93704\n",
      "  67.50165    133.91675     37.44707     11.003129    27.395025\n",
      " 126.76779      0.         154.08922     66.38691    128.32011\n",
      "  29.352787    11.595778    24.178328   117.0287       0.\n",
      " 152.44188     60.384296   123.15838     27.452053     9.1131735\n",
      "  21.216295   101.46893      0.         130.27237     52.69319\n",
      " 113.42484     46.251133    15.152869    52.801105   139.65216\n",
      "   0.         174.5212     110.19697    123.846       41.86401\n",
      "   0.           0.         150.45232      0.         103.255745\n",
      "  41.11293    147.20505   ]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add67f1-16ea-4abc-a86c-050104e8c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Constants\n",
    "num_samples = 2500  # Number of samples\n",
    "image_shape = (32, 32, 3)  # Shape of each image\n",
    "num_classes = 10  # Example number of classes (you can adjust as needed)\n",
    "epochs = 10000  # Number of epochs\n",
    "\n",
    "# Simulate image data (random RGB images for demonstration purposes)\n",
    "images = data['data']\n",
    "# Simulate target labels as integers (e.g., class indices)\n",
    "targets = data['targets']\n",
    "# Convert targets to one-hot encoding\n",
    "one_hot_labels = tf.keras.utils.to_categorical(targets, num_classes=num_classes)\n",
    "\n",
    "# Resize images to the required size for ResNet50V2 (224, 224, 3)\n",
    "resized_images = tf.image.resize(images, (224, 224))\n",
    "\n",
    "# Preprocess images for ResNet50V2\n",
    "processed_images = preprocess_input(resized_images)\n",
    "\n",
    "# Load ResNet50V2 model with pre-trained weights from ImageNet\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers for finetuning\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)  # Pool the feature maps to 1D feature vector\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the final model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model for 10,000 epochs\n",
    "print(\"\\nTraining the Model:\")\n",
    "model.fit(\n",
    "    processed_images,\n",
    "    one_hot_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create a feature extractor using the trained base model\n",
    "print(\"\\nExtracting Features:\")\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Extract features for the input images\n",
    "features = feature_extractor.predict(processed_images, batch_size=32)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(f\"Extracted features shape: {features.shape}\")  # Example: (2500, 7, 7, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091d580-da1a-44c2-bc7e-4bb24bc98d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 0us/step\n",
      "\n",
      "Training the model:\n",
      "Epoch 1/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - accuracy: 0.4983 - loss: 1.5220 - learning_rate: 0.0010\n",
      "Epoch 2/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.8779 - loss: 0.3945 - learning_rate: 0.0010\n",
      "Epoch 3/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - accuracy: 0.9213 - loss: 0.2843 - learning_rate: 0.0010\n",
      "Epoch 4/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9495 - loss: 0.2077 - learning_rate: 0.0010\n",
      "Epoch 5/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9633 - loss: 0.1657 - learning_rate: 0.0010\n",
      "Epoch 6/10000\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9726 - loss: 0.1434 - learning_rate: 0.0010\n",
      "Epoch 7/10000\n",
      "\u001b[1m18/79\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 2s/step - accuracy: 0.9956 - loss: 0.1070"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define constants\n",
    "num_samples = 2500  # Number of images\n",
    "image_shape = (32, 32, 3)  # Original image shape\n",
    "num_classes = 10  # Number of classes\n",
    "epochs = 100000  # Number of epochs\n",
    "\n",
    "# Generate synthetic image data for demonstration (random RGB images)\n",
    "images = data['data']\n",
    "\n",
    "# Generate synthetic target labels (random integers in range of num_classes)\n",
    "targets = data['targets']\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "one_hot_labels = tf.keras.utils.to_categorical(targets, num_classes=num_classes)\n",
    "\n",
    "# Resize images to the required input size for ResNet50V2 (224, 224, 3)\n",
    "resized_images = tf.image.resize(images, (224, 224))\n",
    "\n",
    "# Preprocess images using ResNet50V2 preprocessing\n",
    "processed_images = preprocess_input(resized_images)\n",
    "\n",
    "# Load ResNet50V2 as the base model with pre-trained ImageNet weights\n",
    "base_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model to use it as a feature extractor\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)  # Pass input through the base model\n",
    "x = GlobalAveragePooling2D()(x)  # Pool the output feature maps into a single feature vector\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)  # Output layer for classification\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Define callbacks to manage training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model for feature extraction\n",
    "print(\"\\nTraining the model:\")\n",
    "model.fit(\n",
    "    processed_images,\n",
    "    one_hot_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create a feature extractor from the trained ResNet50V2 base model\n",
    "print(\"\\nExtracting features using the base model:\")\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Use the feature extractor to extract features from the images\n",
    "features = feature_extractor.predict(processed_images, batch_size=32)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(f\"Extracted features shape: {features.shape}\")  # Example: (2500, 7, 7, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a4d92-2928-4e68-ac47-c70f4d78781b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
